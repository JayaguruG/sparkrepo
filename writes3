package org.inceptez.spark.aws

import org.apache.spark.sql._

//HDFS/HIVE -> SPARK -> AWS (S3)

object writes3
{
  def main(args:Array[String])
  {
    val spark = SparkSession
      .builder()
      .master("local")
      .appName("s3write")
      .config("hive.metastore.uris", "thrift://localhost:9083")
      .config("spark.sql.warehouse.dir", "/user/hive/warehouse")   
      .enableHiveSupport()
      .getOrCreate()
      spark.sparkContext.setLogLevel("ERROR")
   spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", "ssss")
      spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", "dfdsfsdfsd+WCA")
      spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
      spark.sparkContext.hadoopConfiguration.set("com.amazonaws.services.s3.enableV4", "true")
      spark.sparkContext.hadoopConfiguration.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
       
      val df = spark.read.option("header","false")
       .option("delimiter", ",")
       .option("inferschema", "true")
       .csv("file:/home/hduser/hive/data/txns")
       df.show(false)
       
       df.write.option("mode", "overwrite").csv("s3a://com.iz.bucket2/txnsinceptez4/");
  println("S3 Object is created")
  }
       
}









